# ==============================================================================
# SCRIPT COMPLETO E OTIMIZADO PARA O ASSISTENTE DE PESQUISA
# ==============================================================================

import streamlit as st
import os
import shutil
from dotenv import load_dotenv

# Importa√ß√µes atualizadas do Langchain para criar um Agente
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain.tools.retriever import create_retriever_tool

from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS

# --- CONFIGURA√á√ïES E CONSTANTES ---

# Caminho onde o √≠ndice FAISS vetorial ser√° salvo e carregado
FAISS_INDEX_PATH = "faiss_index"

# --- FUN√á√ïES DE L√ìGICA DO AGENTE OTIMIZADAS ---

def configure_llm_and_embeddings(api_key):
    """Inicializa o modelo LLM (GPT) e os Embeddings com a chave de API da OpenAI."""
    try:
        llm = ChatOpenAI(model="gpt-3.5-turbo", api_key=api_key, max_tokens=1500, temperature=0.7)
        embeddings = OpenAIEmbeddings(api_key=api_key)
        return llm, embeddings
    except Exception as e:
        st.error(f"Erro ao inicializar os modelos da OpenAI: {e}")
        return None, None

def create_and_save_vector_store(embeddings_model):
    """
    L√™ os arquivos da pasta 'docs', cria a base de conhecimento vetorial (Vector Store)
    e a salva em disco para uso futuro.
    """
    docs_path = "docs"
    if not os.path.exists(docs_path):
        st.sidebar.error(f"A pasta '{docs_path}' n√£o foi encontrada.")
        return None

    documents, failed_files = [], []
    total_files_processed = 0
    
    spinner_text = 'Lendo e processando arquivos da pasta "docs" para criar uma nova base de conhecimento...'
    with st.spinner(spinner_text):
        for dirpath, _, filenames in os.walk(docs_path):
            for file_name in filenames:
                file_path = os.path.join(dirpath, file_name)
                try:
                    loader = None
                    if file_name.endswith('.pdf'):
                        loader = PyPDFLoader(file_path)
                    elif file_name.endswith('.docx'):
                        loader = UnstructuredWordDocumentLoader(file_path)
                    elif file_name.endswith('.pptx'):
                        loader = UnstructuredPowerPointLoader(file_path)
                    
                    if loader:
                        documents.extend(loader.load())
                        total_files_processed += 1
                except Exception as e:
                    failed_files.append((file_name, str(e)))
    
    for file_name, error_msg in failed_files:
        st.sidebar.error(f"Erro ao processar '{file_name}'. O arquivo foi ignorado.")

    if not documents:
        st.sidebar.warning("Nenhum documento foi encontrado ou p√¥de ser processado com sucesso.")
        return None

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=250)
    texts = text_splitter.split_documents(documents)
    
    # Cria o Vector Store a partir dos documentos
    vector_store = FAISS.from_documents(texts, embeddings_model)
    
    # Salva o Vector Store localmente no caminho definido
    vector_store.save_local(FAISS_INDEX_PATH)
    
    st.sidebar.success(f"Nova base de conhecimento criada e salva com {total_files_processed} arquivo(s).")
    return vector_store

def load_or_create_vector_store(embeddings_model):
    """
    Carrega o Vector Store do disco se ele existir.
    Caso contr√°rio, cria um novo e o salva no disco.
    Utiliza o cache do Streamlit para evitar recarregamentos desnecess√°rios durante a mesma sess√£o.
    """
    # Usamos o cache para esta fun√ß√£o, garantindo que ela rode apenas uma vez por sess√£o.
    @st.cache_resource(show_spinner="Carregando base de conhecimento...")
    def _load_or_create_cached():
        if os.path.exists(FAISS_INDEX_PATH):
            st.sidebar.info("Carregando base de conhecimento existente do disco.")
            # O par√¢metro allow_dangerous_deserialization √© necess√°rio para o FAISS
            return FAISS.load_local(FAISS_INDEX_PATH, embeddings_model, allow_dangerous_deserialization=True)
        else:
            st.sidebar.info("Nenhuma base de conhecimento encontrada. Criando uma nova.")
            return create_and_save_vector_store(embeddings_model)
            
    return _load_or_create_cached()

# --- INTERFACE GR√ÅFICA COM STREAMLIT ---

load_dotenv()
st.set_page_config(page_title="Assistente Criativo de Pesquisa", layout="wide")

st.markdown("""
<style>
    [data-testid="stAppViewContainer"] > .main { background-color: #f0f2f6; }
    [data-testid="stChatInput"] textarea { height: 100px; font-size: 16px; border-radius: 10px; }
    h1 { color: #1E3A8A; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

st.title("ü§ñ Assistente Criativo de Pesquisa com GPT")
st.write("Pergunte-me qualquer coisa! Eu pesquisarei nos seus documentos e na internet para fornecer respostas completas e criativas.")

with st.sidebar:
    st.header("Base de Conhecimento")
    st.info(f"A base de dados √© carregada da pasta local `{FAISS_INDEX_PATH}`.")
    
    if st.button("Reconstruir Base de Conhecimento"):
        if os.path.exists(FAISS_INDEX_PATH):
            with st.spinner("Removendo base de conhecimento antiga..."):
                shutil.rmtree(FAISS_INDEX_PATH)
            st.cache_resource.clear()  # Limpa todo o cache de recursos do Streamlit
            st.success("Base de conhecimento antiga removida! A aplica√ß√£o ser√° reiniciada para criar uma nova.")
            st.rerun()  # For√ßa a reinicializa√ß√£o do script
        else:
            st.warning("Nenhuma base de conhecimento para remover. A aplica√ß√£o ser√° reiniciada para criar uma.")
            st.cache_resource.clear()
            st.rerun()

    st.header("Conex√£o com a Internet")
    st.success("Este agente usa a API Tavily para buscar informa√ß√µes atualizadas na web.")

# --- L√ìGICA PRINCIPAL DO AGENTE ---

openai_api_key = os.getenv("OPENAI_API_KEY")
tavily_api_key = os.getenv("TAVILY_API_KEY")

if not openai_api_key or not tavily_api_key:
    st.error("Chaves de API n√£o encontradas!")
    st.info("Por favor, configure as vari√°veis `OPENAI_API_KEY` e `TAVILY_API_KEY` no seu arquivo .env.")
    st.stop()

llm, embeddings = configure_llm_and_embeddings(openai_api_key)
if llm is None or embeddings is None:
    st.stop()

# MODIFICADO: Chamamos a nova fun√ß√£o que gerencia o carregamento ou cria√ß√£o da base
vector_store = load_or_create_vector_store(embeddings)

if vector_store is None:
    st.warning("A base de conhecimento n√£o p√¥de ser carregada ou criada. A busca em documentos est√° desativada.")
    tools = [TavilySearchResults(max_results=3, tavily_api_key=tavily_api_key)]
else:
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})
    retriever_tool = create_retriever_tool(
        retriever,
        "document_search",
        "Busca informa√ß√µes na base de conhecimento local. Use esta ferramenta para qualquer pergunta sobre os arquivos fornecidos."
    )
    # A lista de ferramentas agora depende da exist√™ncia do vector_store
    tools = [retriever_tool, TavilySearchResults(max_results=3, tavily_api_key=tavily_api_key)]

# Cria o prompt do agente
prompt = ChatPromptTemplate.from_messages([
    ("system", """Voc√™ √© um assistente de pesquisa IA avan√ßado e criativo.
Sua miss√£o √© fornecer respostas longas, detalhadas e bem estruturadas.
Estrat√©gia:
1.  **Priorize a ferramenta `document_search`** para responder com base nos arquivos fornecidos.
2.  Se a informa√ß√£o n√£o estiver nos documentos, use a `tavily_search_results_json` para pesquisar na internet.
3.  Sintetize as informa√ß√µes de ambas as fontes, se necess√°rio, para criar a resposta mais completa poss√≠vel.
4.  Seja criativo e anal√≠tico. N√£o se limite a repetir a informa√ß√£o; explique, conecte ideias e ofere√ßa insights.
5.  Sempre cite suas fontes, seja o nome do arquivo local ou o link da web."""),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"),
])

agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# L√≥gica do Chat
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ol√°! Estou pronto para pesquisar e criar. Qual √© a sua pergunta?"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_prompt := st.chat_input("Digite sua pergunta aqui..."):
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    with st.chat_message("assistant"):
        with st.spinner("Pesquisando e elaborando a resposta..."):
            response = agent_executor.invoke({"input": user_prompt})
            response_text = response['output']
            st.markdown(response_text)
    
    st.session_state.messages.append({"role": "assistant", "content": response_text})