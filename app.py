# ==============================================================================
# SCRIPT COMPLETO E OTIMIZADO PARA O ASSISTENTE DE PESQUISA (COM MEM√ìRIA)
# ==============================================================================

import streamlit as st
import os
import shutil
from dotenv import load_dotenv

# Importa√ß√µes atualizadas do Langchain para criar um Agente
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # NOVA IMPORTA√á√ÉO
from langchain.tools.retriever import create_retriever_tool

from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader, UnstructuredPowerPointLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory # NOVA IMPORTA√á√ÉO PARA MEM√ìRIA

# --- CONFIGURA√á√ïES E CONSTANTES ---

# Caminho onde o √≠ndice FAISS vetorial ser√° salvo e carregado
FAISS_INDEX_PATH = "faiss_index"

# --- FUN√á√ïES DE L√ìGICA DO AGENTE OTIMIZADAS ---

def configure_llm_and_embeddings(api_key):
    """Inicializa o modelo LLM (GPT) e os Embeddings com a chave de API da OpenAI."""
    try:
        llm = ChatOpenAI(model="gpt-3.5-turbo", api_key=api_key, max_tokens=1500, temperature=0.7)
        embeddings = OpenAIEmbeddings(api_key=api_key)
        return llm, embeddings
    except Exception as e:
        st.error(f"Erro ao inicializar os modelos da OpenAI: {e}")
        return None, None

def create_and_save_vector_store(embeddings_model):
    """
    L√™ os arquivos da pasta 'docs', cria a base de conhecimento vetorial (Vector Store)
    e a salva em disco para uso futuro.
    """
    docs_path = "docs"
    if not os.path.exists(docs_path):
        st.sidebar.error(f"A pasta '{docs_path}' n√£o foi encontrada.")
        return None

    documents, failed_files = [], []
    total_files_processed = 0
    
    spinner_text = 'Lendo e processando arquivos da pasta "docs" para criar uma nova base de conhecimento...'
    with st.spinner(spinner_text):
        for dirpath, _, filenames in os.walk(docs_path):
            for file_name in filenames:
                file_path = os.path.join(dirpath, file_name)
                try:
                    loader = None
                    if file_name.endswith('.pdf'):
                        loader = PyPDFLoader(file_path)
                    elif file_name.endswith('.docx'):
                        loader = UnstructuredWordDocumentLoader(file_path)
                    elif file_name.endswith('.pptx'):
                        loader = UnstructuredPowerPointLoader(file_path)
                    
                    if loader:
                        documents.extend(loader.load())
                        total_files_processed += 1
                except Exception as e:
                    failed_files.append((file_name, str(e)))
    
    for file_name, error_msg in failed_files:
        st.sidebar.error(f"Erro ao processar '{file_name}'. O arquivo foi ignorado.")

    if not documents:
        st.sidebar.warning("Nenhum documento foi encontrado ou p√¥de ser processado com sucesso.")
        return None

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=250)
    texts = text_splitter.split_documents(documents)
    
    # Cria o Vector Store a partir dos documentos
    vector_store = FAISS.from_documents(texts, embeddings_model)
    
    # Salva o Vector Store localmente no caminho definido
    vector_store.save_local(FAISS_INDEX_PATH)
    
    st.sidebar.success(f"Nova base de conhecimento criada e salva com {total_files_processed} arquivo(s).")
    return vector_store

def load_or_create_vector_store(embeddings_model):
    """
    Carrega o Vector Store do disco se ele existir.
    Caso contr√°rio, cria um novo e o salva no disco.
    Utiliza o cache do Streamlit para evitar recarregamentos desnecess√°rios durante a mesma sess√£o.
    """
    @st.cache_resource(show_spinner="Carregando base de conhecimento...")
    def _load_or_create_cached():
        if os.path.exists(FAISS_INDEX_PATH):
            st.sidebar.info("Carregando base de conhecimento existente do disco.")
            return FAISS.load_local(FAISS_INDEX_PATH, embeddings_model, allow_dangerous_deserialization=True)
        else:
            st.sidebar.info("Nenhuma base de conhecimento encontrada. Criando uma nova.")
            return create_and_save_vector_store(embeddings_model)
            
    return _load_or_create_cached()

# --- INTERFACE GR√ÅFICA COM STREAMLIT ---

load_dotenv()
st.set_page_config(page_title="Assistente Criativo de Pesquisa", layout="wide")

st.markdown("""
<style>
    [data-testid="stAppViewContainer"] > .main { background-color: #f0f2f6; }
    [data-testid="stChatInput"] textarea { height: 100px; font-size: 16px; border-radius: 10px; }
    h1 { color: #1E3A8A; font-weight: bold; }
</style>
""", unsafe_allow_html=True)

st.title("ü§ñ Assistente Criativo de Pesquisa com GPT")
st.write("Pergunte-me qualquer coisa! Eu pesquisarei nos seus documentos e na internet para fornecer respostas completas e criativas.")

with st.sidebar:
    st.header("Base de Conhecimento")
    st.info(f"A base de dados √© carregada da pasta local `{FAISS_INDEX_PATH}`.")
    
    if st.button("Reconstruir Base de Conhecimento"):
        if os.path.exists(FAISS_INDEX_PATH):
            with st.spinner("Removendo base de conhecimento antiga..."):
                shutil.rmtree(FAISS_INDEX_PATH)
            st.cache_resource.clear()
            st.success("Base de conhecimento antiga removida! A aplica√ß√£o ser√° reiniciada para criar uma nova.")
            st.rerun()
        else:
            st.warning("Nenhuma base de conhecimento para remover. A aplica√ß√£o ser√° reiniciada para criar uma.")
            st.cache_resource.clear()
            st.rerun()

    st.header("Conex√£o com a Internet")
    st.success("Este agente usa a API Tavily para buscar informa√ß√µes atualizadas na web.")

# --- L√ìGICA PRINCIPAL DO AGENTE ---

openai_api_key = os.getenv("OPENAI_API_KEY")
tavily_api_key = os.getenv("TAVILY_API_KEY")

if not openai_api_key or not tavily_api_key:
    st.error("Chaves de API n√£o encontradas!")
    st.info("Por favor, configure as vari√°veis `OPENAI_API_KEY` e `TAVILY_API_KEY` no seu arquivo .env.")
    st.stop()

llm, embeddings = configure_llm_and_embeddings(openai_api_key)
if llm is None or embeddings is None:
    st.stop()

vector_store = load_or_create_vector_store(embeddings)

if vector_store is None:
    st.warning("A base de conhecimento n√£o p√¥de ser carregada ou criada. A busca em documentos est√° desativada.")
    tools = [TavilySearchResults(max_results=3, tavily_api_key=tavily_api_key)]
else:
    retriever = vector_store.as_retriever(search_kwargs={"k": 5})
    retriever_tool = create_retriever_tool(
        retriever,
        "document_search",
        "Busca informa√ß√µes na base de conhecimento local. Use esta ferramenta para qualquer pergunta sobre os arquivos fornecidos."
    )
    tools = [retriever_tool, TavilySearchResults(max_results=3, tavily_api_key=tavily_api_key)]

# ======================= NOVIDADE: IN√çCIO DA SE√á√ÉO DE MEM√ìRIA =======================
# Adicionamos um placeholder para o hist√≥rico do chat no prompt.
# O `MessagesPlaceholder` √© uma parte especial do prompt que dir√° ao Langchain onde
# injetar o hist√≥rico da conversa.
prompt = ChatPromptTemplate.from_messages([
    ("system", """Voc√™ √© um assistente de pesquisa IA avan√ßado e criativo.
Sua miss√£o √© fornecer respostas longas, detalhadas e bem estruturadas.
Estrat√©gia:
1.  **Priorize a ferramenta `document_search`** para responder com base nos arquivos fornecidos.
2.  Se a informa√ß√£o n√£o estiver nos documentos, use a `tavily_search_results_json` para pesquisar na internet.
3.  Sintetize as informa√ß√µes de ambas as fontes, se necess√°rio, para criar a resposta mais completa poss√≠vel.
4.  Seja criativo e anal√≠tico. N√£o se limite a repetir a informa√ß√£o; explique, conecte ideias e ofere√ßa insights.
5.  Sempre cite suas fontes, seja o nome do arquivo local ou o link da web."""),
    MessagesPlaceholder(variable_name="chat_history"), # Onde o hist√≥rico ser√° inserido
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"),
])

# Inicializa a mem√≥ria. Usamos a ConversationBufferMemory que armazena as mensagens.
# `memory_key` deve ser o mesmo que `variable_name` no MessagesPlaceholder.
# `return_messages=True` garante que a mem√≥ria retorne os objetos de mensagem do Langchain.
if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ======================= NOVIDADE: FIM DA SE√á√ÉO DE MEM√ìRIA =========================

agent = create_openai_tools_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# L√≥gica do Chat
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ol√°! Estou pronto para pesquisar e criar. Qual √© a sua pergunta?"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_prompt := st.chat_input("Digite sua pergunta aqui..."):
    st.session_state.messages.append({"role": "user", "content": user_prompt})
    with st.chat_message("user"):
        st.markdown(user_prompt)

    with st.chat_message("assistant"):
        with st.spinner("Pesquisando e elaborando a resposta..."):
            # ======================= NOVIDADE: INCLUS√ÉO DA MEM√ìRIA NA CHAMADA =======================
            # Carregamos o hist√≥rico da mem√≥ria e o inclu√≠mos na chamada `invoke`.
            # O hist√≥rico ser√° inserido no `MessagesPlaceholder` do prompt.
            chat_history = st.session_state.memory.load_memory_variables({})["chat_history"]
            response = agent_executor.invoke({
                "input": user_prompt,
                "chat_history": chat_history
            })
            response_text = response['output']
            
            # Salvamos a intera√ß√£o atual (pergunta do usu√°rio e resposta do agente) na mem√≥ria.
            st.session_state.memory.save_context(
                {"input": user_prompt}, 
                {"output": response_text}
            )
            # ======================================================================================

            st.markdown(response_text)
    
    st.session_state.messages.append({"role": "assistant", "content": response_text})